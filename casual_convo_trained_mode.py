# -*- coding: utf-8 -*-
"""Casual Convo Trained Mode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115ZT0JkXE-5buW-hoSt6kAPubTStY1Yo
"""

# Cell 1 — install/upgrade libs (run once)
!pip install -qU "transformers" "datasets" "accelerate" "evaluate" "huggingface_hub" "sentencepiece"
!pip install -qU peft

# Cell 2 — imports + config
import os, math, random
import torch
from datasets import load_dataset, concatenate_datasets
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)

# Config / hyperparams
CFG = {
    "model_name": "facebook/blenderbot-400M-distill",
    "persona_dataset": "awsaf49/persona-chat",
    "casual_dataset": "SohamGhadge/casual-conversation",
    "context_turns": 3,
    "max_input_length": 256,
    "max_target_length": 128,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "num_train_epochs": 3,
    "learning_rate": 5e-5,
    "eval_steps": 1000,
    "logging_steps": 200,
    "save_steps": 1000
}

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# Cell 3 — load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(CFG["model_name"], use_fast=True)
tokenizer.padding_side = "right"
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForSeq2SeqLM.from_pretrained(CFG["model_name"]).to(device)

# Cell 4 — load datasets
persona = load_dataset(CFG["persona_dataset"])
casual = load_dataset(CFG["casual_dataset"])

print("Persona splits:", list(persona.keys()))
print("Casual splits:", list(casual.keys()))

# Cell 5 — preprocess helpers
def persona_preprocess_batch(batch, context_turns=3):
    inputs, targets = [], []
    # Ensure personalities, histories, and utterances are lists, even if keys are missing
    personalities = batch.get("personality", [])
    histories = batch.get("history", []) or batch.get("utterances", []) or batch.get("conversations", [])
    utterances = batch.get("utterance", []) or batch.get("response", []) or batch.get("target", [])
    for p, h, u in zip(personalities, histories, utterances):
        persona_str = "Persona: " + " | ".join(p) if p else ""
        ctx = "\n".join(h[-context_turns:]) if isinstance(h, list) else (str(h) if h else "")
        input_text = (persona_str + "\n" + ctx).strip() if persona_str else ctx.strip()
        target_text = u if isinstance(u, str) else (u[0] if isinstance(u, list) and u else "")
        inputs.append(input_text.strip())
        targets.append(target_text.strip())
    return {"input_text": inputs, "target_text": targets}

def casual_preprocess_batch(batch):
    q = batch.get("question") or batch.get("prompt") or batch.get("input") or []
    a = batch.get("answer") or batch.get("response") or batch.get("target") or []
    inputs = [str(x).strip() if x else "" for x in q]
    targets = [str(x).strip() if x else "" for x in a]
    return {"input_text": inputs, "target_text": targets}

def clean_dataset(ds):
    # remove NaN, None, empty
    ds = ds.filter(lambda ex: ex["input_text"] and ex["target_text"])
    # remove whitespace-only
    ds = ds.filter(lambda ex: ex["input_text"].strip() != "" and ex["target_text"].strip() != "")
    # deduplicate on both input+target
    ds = ds.drop_duplicates(subset=["input_text", "target_text"])
    return ds

def tokenize_batch(examples):
    model_inputs = tokenizer(examples["input_text"],
                             max_length=CFG["max_input_length"],
                             truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["target_text"],
                           max_length=CFG["max_target_length"],
                           truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Cell 6 — map preprocessing, cleaning, tokenizing
persona_tokenized_splits = {}
for split in persona.keys():
    mapped = persona[split].map(
        lambda batch: persona_preprocess_batch(batch, context_turns=CFG["context_turns"]),
        batched=True,
        remove_columns=persona[split].column_names
    )
    mapped = clean_dataset(mapped)
    mapped = mapped.map(tokenize_batch, batched=True, remove_columns=["input_text", "target_text"])
    persona_tokenized_splits[split] = mapped

casual_tokenized_splits = {}
for split in casual.keys():
    mapped = casual[split].map(
        casual_preprocess_batch,
        batched=True,
        remove_columns=casual[split].column_names
    )
    mapped = clean_dataset(mapped)
    mapped = mapped.map(tokenize_batch, batched=True, remove_columns=["input_text", "target_text"])
    casual_tokenized_splits[split] = mapped

# Cell 7 — combine train/eval sets
train_datasets = []
if "train" in persona_tokenized_splits:
    train_datasets.append(persona_tokenized_splits["train"])
if "train" in casual_tokenized_splits:
    train_datasets.append(casual_tokenized_splits["train"])
if not train_datasets:
    raise ValueError("No training splits found.")

train_dataset = concatenate_datasets(train_datasets)
print("Train size after cleaning:", len(train_dataset))

eval_dataset = None
if "validation" in persona_tokenized_splits:
    eval_dataset = persona_tokenized_splits["validation"]
elif "valid" in persona_tokenized_splits:
    eval_dataset = persona_tokenized_splits["valid"]
elif "test" in persona_tokenized_splits:
    eval_dataset = persona_tokenized_splits["test"]
else:
    eval_dataset = train_dataset.select(range(min(2000, len(train_dataset))))

# Cell 8 — trainer setup
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)

training_args = Seq2SeqTrainingArguments(
    output_dir="./blenderbot_persona_casual_cleaned",
    per_device_train_batch_size=CFG["per_device_train_batch_size"],
    per_device_eval_batch_size=CFG["per_device_eval_batch_size"],
    predict_with_generate=True,
    evaluation_strategy="steps",
    eval_steps=CFG["eval_steps"],
    logging_steps=CFG["logging_steps"],
    save_steps=CFG["save_steps"],
    save_total_limit=3,
    num_train_epochs=CFG["num_train_epochs"],
    learning_rate=CFG["learning_rate"],
    fp16=torch.cuda.is_available(),
    remove_unused_columns=True,
    push_to_hub=False,
    load_best_model_at_end=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

# Cell 9 — train
trainer.train()

# Cell 10 — inference helper
def chat_with_model(prompt, max_new_tokens=128):
    model.eval()
    inputs = tokenizer(prompt, return_tensors="pt",
                       truncation=True, max_length=CFG["max_input_length"]).to(device)
    out = model.generate(**inputs, max_new_tokens=max_new_tokens,
                         do_sample=True, top_p=0.9, temperature=0.8)
    return tokenizer.decode(out[0], skip_special_tokens=True)

# Example:
# print(chat_with_model("Persona: I love fashion\nHey, how was your day?"))